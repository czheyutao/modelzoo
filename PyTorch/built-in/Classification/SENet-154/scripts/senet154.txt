--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
/root/miniconda3/envs/torch_env_py310/lib/python3.10/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0217 03:18:02.836000 139994603992896 torch/distributed/run.py:779] 
W0217 03:18:02.836000 139994603992896 torch/distributed/run.py:779] *****************************************
W0217 03:18:02.836000 139994603992896 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0217 03:18:02.836000 139994603992896 torch/distributed/run.py:779] *****************************************
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Model senet154 created, param count: 115088984
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bilinear
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
NVIDIA APEX not installed. AMP off.
Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP.
Scheduled epochs: 150
Train: 0 [   0/10009 (  0%)]  Loss:  7.004124 (7.0041)  Time: 231.735s,    0.55/s  (231.735s,    0.55/s)  LR: 1.000e-04  Data: 4.603 (4.603)
Train: 0 [   1/10009 (  0%)]  Loss:  6.970666 (6.9874)  Time: 226.851s,    0.56/s  (229.293s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (2.304)
Train: 0 [   2/10009 (  0%)]  Loss:  7.014361 (6.9964)  Time: 226.848s,    0.56/s  (228.478s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (1.538)
Train: 0 [   3/10009 (  0%)]  Loss:  7.010243 (6.9998)  Time: 226.850s,    0.56/s  (228.071s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (1.155)
Train: 0 [   4/10009 (  0%)]  Loss:  6.983081 (6.9965)  Time: 226.850s,    0.56/s  (227.827s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.925)
Train: 0 [   5/10009 (  0%)]  Loss:  6.966183 (6.9914)  Time: 226.857s,    0.56/s  (227.665s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.772)
Train: 0 [   6/10009 (  0%)]  Loss:  7.015144 (6.9948)  Time: 226.850s,    0.56/s  (227.549s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.662)
Train: 0 [   7/10009 (  0%)]  Loss:  7.061026 (7.0031)  Time: 226.850s,    0.56/s  (227.461s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.580)
Train: 0 [   8/10009 (  0%)]  Loss:  6.981637 (7.0007)  Time: 226.850s,    0.56/s  (227.393s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.516)
Train: 0 [   9/10009 (  0%)]  Loss:  6.992671 (6.9999)  Time: 226.850s,    0.56/s  (227.339s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.465)
Train: 0 [  10/10009 (  0%)]  Loss:  6.940483 (6.9945)  Time: 226.849s,    0.56/s  (227.295s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.424)
Train: 0 [  11/10009 (  0%)]  Loss:  7.008171 (6.9956)  Time: 226.849s,    0.56/s  (227.257s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.389)
Train: 0 [  12/10009 (  0%)]  Loss:  7.026023 (6.9980)  Time: 226.849s,    0.56/s  (227.226s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.359)
Train: 0 [  13/10009 (  0%)]  Loss:  6.978179 (6.9966)  Time: 226.850s,    0.56/s  (227.199s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.334)
Train: 0 [  14/10009 (  0%)]  Loss:  6.962049 (6.9943)  Time: 226.849s,    0.56/s  (227.176s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.312)
Train: 0 [  15/10009 (  0%)]  Loss:  6.966998 (6.9926)  Time: 226.849s,    0.56/s  (227.155s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.293)
Train: 0 [  16/10009 (  0%)]  Loss:  6.979096 (6.9918)  Time: 226.850s,    0.56/s  (227.137s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.276)
Train: 0 [  17/10009 (  0%)]  Loss:  7.020649 (6.9934)  Time: 226.849s,    0.56/s  (227.121s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.261)
Train: 0 [  18/10009 (  0%)]  Loss:  7.011188 (6.9943)  Time: 226.849s,    0.56/s  (227.107s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.248)
Train: 0 [  19/10009 (  0%)]  Loss:  7.032535 (6.9962)  Time: 226.850s,    0.56/s  (227.094s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.236)
Train: 0 [  20/10009 (  0%)]  Loss:  7.007079 (6.9967)  Time: 226.853s,    0.56/s  (227.083s,    0.56/s)  LR: 1.000e-04  Data: 0.008 (0.225)
Train: 0 [  21/10009 (  0%)]  Loss:  6.975900 (6.9958)  Time: 226.849s,    0.56/s  (227.072s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.215)
Train: 0 [  22/10009 (  0%)]  Loss:  6.983974 (6.9953)  Time: 226.849s,    0.56/s  (227.062s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.206)
Train: 0 [  23/10009 (  0%)]  Loss:  6.990951 (6.9951)  Time: 226.848s,    0.56/s  (227.053s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.197)
Train: 0 [  24/10009 (  0%)]  Loss:  6.974842 (6.9943)  Time: 226.849s,    0.56/s  (227.045s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.190)
Train: 0 [  25/10009 (  0%)]  Loss:  7.023876 (6.9954)  Time: 226.850s,    0.56/s  (227.038s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.183)
Train: 0 [  26/10009 (  0%)]  Loss:  7.000998 (6.9956)  Time: 226.849s,    0.56/s  (227.031s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.176)
Train: 0 [  27/10009 (  0%)]  Loss:  6.959955 (6.9944)  Time: 226.850s,    0.56/s  (227.024s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.170)
Train: 0 [  28/10009 (  0%)]  Loss:  6.961563 (6.9932)  Time: 226.849s,    0.56/s  (227.018s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.164)
Train: 0 [  29/10009 (  0%)]  Loss:  7.009366 (6.9938)  Time: 226.849s,    0.56/s  (227.013s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.159)
Train: 0 [  30/10009 (  0%)]  Loss:  6.959389 (6.9927)  Time: 226.848s,    0.56/s  (227.007s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.154)
Train: 0 [  31/10009 (  0%)]  Loss:  6.984647 (6.9924)  Time: 226.849s,    0.56/s  (227.002s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.149)
Train: 0 [  32/10009 (  0%)]  Loss:  7.000431 (6.9927)  Time: 226.849s,    0.56/s  (226.998s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.145)
Train: 0 [  33/10009 (  0%)]  Loss:  6.960710 (6.9917)  Time: 226.849s,    0.56/s  (226.993s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.141)
Train: 0 [  34/10009 (  0%)]  Loss:  6.988856 (6.9916)  Time: 226.849s,    0.56/s  (226.989s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.137)
Train: 0 [  35/10009 (  0%)]  Loss:  7.011896 (6.9922)  Time: 226.850s,    0.56/s  (226.985s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.133)
Train: 0 [  36/10009 (  0%)]  Loss:  6.927845 (6.9905)  Time: 226.849s,    0.56/s  (226.982s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.130)
Train: 0 [  37/10009 (  0%)]  Loss:  6.953796 (6.9895)  Time: 226.848s,    0.56/s  (226.978s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.127)
Train: 0 [  38/10009 (  0%)]  Loss:  6.985545 (6.9894)  Time: 226.849s,    0.56/s  (226.975s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.124)
Train: 0 [  39/10009 (  0%)]  Loss:  6.992636 (6.9895)  Time: 226.849s,    0.56/s  (226.972s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.121)
Train: 0 [  40/10009 (  0%)]  Loss:  6.984478 (6.9893)  Time: 226.849s,    0.56/s  (226.969s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.118)
Train: 0 [  41/10009 (  0%)]  Loss:  6.957385 (6.9886)  Time: 226.849s,    0.56/s  (226.966s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.115)
Train: 0 [  42/10009 (  0%)]  Loss:  6.955868 (6.9878)  Time: 226.849s,    0.56/s  (226.963s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.113)
Train: 0 [  43/10009 (  0%)]  Loss:  6.993980 (6.9880)  Time: 226.848s,    0.56/s  (226.960s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.110)
Train: 0 [  44/10009 (  0%)]  Loss:  6.990395 (6.9880)  Time: 226.850s,    0.56/s  (226.958s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.108)
Train: 0 [  45/10009 (  0%)]  Loss:  7.026927 (6.9889)  Time: 226.849s,    0.56/s  (226.956s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.106)
Train: 0 [  46/10009 (  0%)]  Loss:  7.004726 (6.9892)  Time: 226.849s,    0.56/s  (226.953s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.104)
Train: 0 [  47/10009 (  0%)]  Loss:  7.027875 (6.9900)  Time: 226.849s,    0.56/s  (226.951s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.102)
Train: 0 [  48/10009 (  0%)]  Loss:  6.971566 (6.9896)  Time: 226.849s,    0.56/s  (226.949s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.100)
Train: 0 [  49/10009 (  0%)]  Loss:  6.991015 (6.9897)  Time: 226.849s,    0.56/s  (226.947s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.098)
Train: 0 [  50/10009 (  0%)]  Loss:  6.954612 (6.9890)  Time: 226.850s,    0.56/s  (226.945s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.096)
Train: 0 [  51/10009 (  1%)]  Loss:  7.052188 (6.9902)  Time: 226.850s,    0.56/s  (226.943s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.094)
Train: 0 [  52/10009 (  1%)]  Loss:  6.956193 (6.9895)  Time: 226.854s,    0.56/s  (226.942s,    0.56/s)  LR: 1.000e-04  Data: 0.008 (0.093)
Train: 0 [  53/10009 (  1%)]  Loss:  7.012786 (6.9900)  Time: 226.851s,    0.56/s  (226.940s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.091)
Train: 0 [  54/10009 (  1%)]  Loss:  7.021721 (6.9906)  Time: 226.849s,    0.56/s  (226.938s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.089)
Train: 0 [  55/10009 (  1%)]  Loss:  6.966161 (6.9901)  Time: 226.848s,    0.56/s  (226.937s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.088)
Train: 0 [  56/10009 (  1%)]  Loss:  6.996459 (6.9902)  Time: 226.853s,    0.56/s  (226.935s,    0.56/s)  LR: 1.000e-04  Data: 0.008 (0.087)
Train: 0 [  57/10009 (  1%)]  Loss:  6.976745 (6.9900)  Time: 226.849s,    0.56/s  (226.934s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.085)
Train: 0 [  58/10009 (  1%)]  Loss:  6.982141 (6.9899)  Time: 226.850s,    0.56/s  (226.932s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.084)
Train: 0 [  59/10009 (  1%)]  Loss:  6.949982 (6.9892)  Time: 226.849s,    0.56/s  (226.931s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.082)
Train: 0 [  60/10009 (  1%)]  Loss:  7.000940 (6.9894)  Time: 226.849s,    0.56/s  (226.930s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.081)
Train: 0 [  61/10009 (  1%)]  Loss:  6.992007 (6.9894)  Time: 226.849s,    0.56/s  (226.928s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.080)
Train: 0 [  62/10009 (  1%)]  Loss:  7.044215 (6.9903)  Time: 226.849s,    0.56/s  (226.927s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.079)
Train: 0 [  63/10009 (  1%)]  Loss:  7.043487 (6.9911)  Time: 226.850s,    0.56/s  (226.926s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.078)
Train: 0 [  64/10009 (  1%)]  Loss:  6.994876 (6.9912)  Time: 226.850s,    0.56/s  (226.925s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.077)
Train: 0 [  65/10009 (  1%)]  Loss:  6.975888 (6.9910)  Time: 226.848s,    0.56/s  (226.924s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.075)
Train: 0 [  66/10009 (  1%)]  Loss:  6.997032 (6.9911)  Time: 226.849s,    0.56/s  (226.922s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.074)
Train: 0 [  67/10009 (  1%)]  Loss:  7.007053 (6.9913)  Time: 226.849s,    0.56/s  (226.921s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.073)
Train: 0 [  68/10009 (  1%)]  Loss:  6.999412 (6.9914)  Time: 226.848s,    0.56/s  (226.920s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.072)
Train: 0 [  69/10009 (  1%)]  Loss:  7.046378 (6.9922)  Time: 226.848s,    0.56/s  (226.919s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.072)
Train: 0 [  70/10009 (  1%)]  Loss:  6.936152 (6.9914)  Time: 226.849s,    0.56/s  (226.918s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.071)
Train: 0 [  71/10009 (  1%)]  Loss:  6.972276 (6.9911)  Time: 226.850s,    0.56/s  (226.917s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.070)
Train: 0 [  72/10009 (  1%)]  Loss:  6.983388 (6.9910)  Time: 226.849s,    0.56/s  (226.916s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.069)
Train: 0 [  73/10009 (  1%)]  Loss:  6.951684 (6.9905)  Time: 226.850s,    0.56/s  (226.916s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.068)
Train: 0 [  74/10009 (  1%)]  Loss:  6.975294 (6.9903)  Time: 226.851s,    0.56/s  (226.915s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.067)
Train: 0 [  75/10009 (  1%)]  Loss:  6.969432 (6.9900)  Time: 226.849s,    0.56/s  (226.914s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.066)
Train: 0 [  76/10009 (  1%)]  Loss:  7.002777 (6.9902)  Time: 226.850s,    0.56/s  (226.913s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.066)
Train: 0 [  77/10009 (  1%)]  Loss:  7.019635 (6.9906)  Time: 226.849s,    0.56/s  (226.912s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.065)
Train: 0 [  78/10009 (  1%)]  Loss:  6.971299 (6.9903)  Time: 226.849s,    0.56/s  (226.911s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.064)
Train: 0 [  79/10009 (  1%)]  Loss:  6.949276 (6.9898)  Time: 226.848s,    0.56/s  (226.911s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.063)
Train: 0 [  80/10009 (  1%)]  Loss:  6.973175 (6.9896)  Time: 226.850s,    0.56/s  (226.910s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.063)
Train: 0 [  81/10009 (  1%)]  Loss:  6.984597 (6.9895)  Time: 226.848s,    0.56/s  (226.909s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.062)
Train: 0 [  82/10009 (  1%)]  Loss:  6.976389 (6.9894)  Time: 226.850s,    0.56/s  (226.908s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.061)
Train: 0 [  83/10009 (  1%)]  Loss:  7.012662 (6.9897)  Time: 226.849s,    0.56/s  (226.908s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.061)
Train: 0 [  84/10009 (  1%)]  Loss:  6.980609 (6.9896)  Time: 226.852s,    0.56/s  (226.907s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.060)
Train: 0 [  85/10009 (  1%)]  Loss:  7.017882 (6.9899)  Time: 226.850s,    0.56/s  (226.906s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.059)
Train: 0 [  86/10009 (  1%)]  Loss:  6.941633 (6.9893)  Time: 226.849s,    0.56/s  (226.906s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.059)
Train: 0 [  87/10009 (  1%)]  Loss:  7.020520 (6.9897)  Time: 226.850s,    0.56/s  (226.905s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.058)
Train: 0 [  88/10009 (  1%)]  Loss:  6.977829 (6.9895)  Time: 226.852s,    0.56/s  (226.904s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.057)
Train: 0 [  89/10009 (  1%)]  Loss:  7.024150 (6.9899)  Time: 226.859s,    0.56/s  (226.904s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.057)
Train: 0 [  90/10009 (  1%)]  Loss:  6.992478 (6.9900)  Time: 226.849s,    0.56/s  (226.903s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.056)
Train: 0 [  91/10009 (  1%)]  Loss:  6.998168 (6.9900)  Time: 226.849s,    0.56/s  (226.903s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.056)
Train: 0 [  92/10009 (  1%)]  Loss:  6.939415 (6.9895)  Time: 226.848s,    0.56/s  (226.902s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.055)
Train: 0 [  93/10009 (  1%)]  Loss:  6.930671 (6.9889)  Time: 226.849s,    0.56/s  (226.902s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.055)
Train: 0 [  94/10009 (  1%)]  Loss:  7.017179 (6.9892)  Time: 226.849s,    0.56/s  (226.901s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.054)
Train: 0 [  95/10009 (  1%)]  Loss:  7.055783 (6.9899)  Time: 226.850s,    0.56/s  (226.900s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.054)
Train: 0 [  96/10009 (  1%)]  Loss:  6.964316 (6.9896)  Time: 226.850s,    0.56/s  (226.900s,    0.56/s)  LR: 1.000e-04  Data: 0.007 (0.053)
Train: 0 [  97/10009 (  1%)]  Loss:  6.985021 (6.9896)  Time: 226.851s,    0.56/s  (226.899s,    0.56/s)  LR: 1.000e-04  Data: 0.006 (0.053)
Train: 0 [  98/10009 (  1%)]  Loss:  6.906923 (6.9887)  Time: 226.853s,    0.56/s  (226.899s,    0.56/s)  LR: 1.000e-04  Data: 0.008 (0.052)
Train: 0 [  99/10009 (  1%)]  Loss:  6.939504 (6.9882)  Time: 226.850s,    0.56/s  (226.898s,    0.56/s)  LR: 1.000e-04  Data: 0.005 (0.052)
