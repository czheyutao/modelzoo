--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
/root/miniconda3/envs/torch_env_py310/lib/python3.10/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0214 07:40:27.920000 140177657976640 torch/distributed/run.py:779] 
W0214 07:40:27.920000 140177657976640 torch/distributed/run.py:779] *****************************************
W0214 07:40:27.920000 140177657976640 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0214 07:40:27.920000 140177657976640 torch/distributed/run.py:779] *****************************************
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Model seresnet34 created, param count: 21958868
Data processing configuration for current model + dataset:
        input_size: (3, 224, 224)
        interpolation: bilinear
        mean: (0.485, 0.456, 0.406)
        std: (0.229, 0.224, 0.225)
        crop_pct: 0.875
NVIDIA APEX not installed. AMP off.
Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP.
Scheduled epochs: 11
Train: 0 [   0/1251 (  0%)]  Loss:  7.071155 (7.0712)  Time: 13.348s,   76.71/s  (13.348s,   76.71/s)  LR: 1.000e-04  Data: 12.519 (12.519)
Train: 0 [   1/1251 (  0%)]  Loss:  7.062699 (7.0669)  Time: 0.856s, 1196.52/s  (7.102s,  144.18/s)  LR: 1.000e-04  Data: 0.030 (6.274)
Train: 0 [   2/1251 (  0%)]  Loss:  7.072742 (7.0689)  Time: 0.854s, 1198.83/s  (5.019s,  204.01/s)  LR: 1.000e-04  Data: 0.030 (4.193)
Train: 0 [   3/1251 (  0%)]  Loss:  7.129293 (7.0840)  Time: 0.934s, 1096.00/s  (3.998s,  256.12/s)  LR: 1.000e-04  Data: 0.034 (3.153)
Train: 0 [   4/1251 (  0%)]  Loss:  7.065943 (7.0804)  Time: 0.864s, 1185.77/s  (3.371s,  303.74/s)  LR: 1.000e-04  Data: 0.040 (2.530)
Train: 0 [   5/1251 (  0%)]  Loss:  7.066686 (7.0781)  Time: 0.856s, 1195.93/s  (2.952s,  346.87/s)  LR: 1.000e-04  Data: 0.032 (2.114)
Train: 0 [   6/1251 (  0%)]  Loss:  7.097903 (7.0809)  Time: 0.852s, 1201.66/s  (2.652s,  386.11/s)  LR: 1.000e-04  Data: 0.031 (1.816)
Train: 0 [   7/1251 (  1%)]  Loss:  7.082118 (7.0811)  Time: 0.855s, 1197.93/s  (2.427s,  421.84/s)  LR: 1.000e-04  Data: 0.032 (1.593)
Train: 0 [   8/1251 (  1%)]  Loss:  7.075727 (7.0805)  Time: 0.858s, 1193.87/s  (2.253s,  454.50/s)  LR: 1.000e-04  Data: 0.032 (1.420)
Train: 0 [   9/1251 (  1%)]  Loss:  7.090921 (7.0815)  Time: 0.854s, 1199.01/s  (2.113s,  484.59/s)  LR: 1.000e-04  Data: 0.033 (1.281)
Train: 0 [  10/1251 (  1%)]  Loss:  7.060435 (7.0796)  Time: 0.853s, 1200.65/s  (1.999s,  512.37/s)  LR: 1.000e-04  Data: 0.032 (1.168)
Train: 0 [  11/1251 (  1%)]  Loss:  7.053773 (7.0774)  Time: 0.857s, 1195.49/s  (1.903s,  537.99/s)  LR: 1.000e-04  Data: 0.033 (1.073)
Train: 0 [  12/1251 (  1%)]  Loss:  7.077629 (7.0775)  Time: 0.858s, 1192.83/s  (1.823s,  561.71/s)  LR: 1.000e-04  Data: 0.032 (0.993)
Train: 0 [  13/1251 (  1%)]  Loss:  7.066471 (7.0767)  Time: 0.852s, 1202.07/s  (1.754s,  583.93/s)  LR: 1.000e-04  Data: 0.032 (0.924)
Train: 0 [  14/1251 (  1%)]  Loss:  7.107177 (7.0787)  Time: 0.854s, 1199.00/s  (1.694s,  604.60/s)  LR: 1.000e-04  Data: 0.033 (0.865)
Train: 0 [  15/1251 (  1%)]  Loss:  7.052311 (7.0771)  Time: 0.855s, 1197.15/s  (1.641s,  623.90/s)  LR: 1.000e-04  Data: 0.035 (0.813)
Train: 0 [  16/1251 (  1%)]  Loss:  7.091420 (7.0779)  Time: 0.854s, 1198.39/s  (1.595s,  642.01/s)  LR: 1.000e-04  Data: 0.032 (0.767)
Train: 0 [  17/1251 (  1%)]  Loss:  7.059393 (7.0769)  Time: 0.860s, 1191.16/s  (1.554s,  658.88/s)  LR: 1.000e-04  Data: 0.033 (0.726)
Train: 0 [  18/1251 (  1%)]  Loss:  7.099226 (7.0781)  Time: 0.859s, 1191.69/s  (1.518s,  674.76/s)  LR: 1.000e-04  Data: 0.039 (0.690)
Train: 0 [  19/1251 (  2%)]  Loss:  7.067566 (7.0775)  Time: 0.852s, 1201.23/s  (1.484s,  689.88/s)  LR: 1.000e-04  Data: 0.032 (0.657)
Train: 0 [  20/1251 (  2%)]  Loss:  7.058359 (7.0766)  Time: 0.855s, 1197.85/s  (1.454s,  704.10/s)  LR: 1.000e-04  Data: 0.031 (0.627)
Train: 0 [  21/1251 (  2%)]  Loss:  7.064164 (7.0761)  Time: 0.859s, 1192.35/s  (1.427s,  717.45/s)  LR: 1.000e-04  Data: 0.031 (0.600)
Train: 0 [  22/1251 (  2%)]  Loss:  7.050086 (7.0749)  Time: 0.854s, 1198.79/s  (1.402s,  730.20/s)  LR: 1.000e-04  Data: 0.031 (0.576)
Train: 0 [  23/1251 (  2%)]  Loss:  7.064773 (7.0745)  Time: 0.864s, 1185.86/s  (1.380s,  742.08/s)  LR: 1.000e-04  Data: 0.044 (0.553)
Train: 0 [  24/1251 (  2%)]  Loss:  7.050241 (7.0735)  Time: 0.864s, 1184.68/s  (1.359s,  753.34/s)  LR: 1.000e-04  Data: 0.035 (0.533)
Train: 0 [  25/1251 (  2%)]  Loss:  7.081606 (7.0738)  Time: 0.857s, 1195.31/s  (1.340s,  764.21/s)  LR: 1.000e-04  Data: 0.035 (0.513)
Train: 0 [  26/1251 (  2%)]  Loss:  7.082312 (7.0742)  Time: 0.858s, 1193.22/s  (1.322s,  774.52/s)  LR: 1.000e-04  Data: 0.034 (0.496)
Train: 0 [  27/1251 (  2%)]  Loss:  7.062420 (7.0737)  Time: 0.856s, 1196.04/s  (1.305s,  784.39/s)  LR: 1.000e-04  Data: 0.032 (0.479)
Train: 0 [  28/1251 (  2%)]  Loss:  7.074733 (7.0738)  Time: 0.860s, 1190.60/s  (1.290s,  793.73/s)  LR: 1.000e-04  Data: 0.032 (0.464)
Train: 0 [  29/1251 (  2%)]  Loss:  7.080493 (7.0740)  Time: 0.859s, 1192.16/s  (1.276s,  802.67/s)  LR: 1.000e-04  Data: 0.032 (0.449)
Train: 0 [  30/1251 (  2%)]  Loss:  7.060961 (7.0736)  Time: 0.854s, 1198.54/s  (1.262s,  811.32/s)  LR: 1.000e-04  Data: 0.031 (0.436)
Train: 0 [  31/1251 (  2%)]  Loss:  7.068716 (7.0734)  Time: 0.860s, 1190.29/s  (1.250s,  819.47/s)  LR: 1.000e-04  Data: 0.036 (0.423)
Train: 0 [  32/1251 (  3%)]  Loss:  7.058354 (7.0730)  Time: 0.857s, 1195.46/s  (1.238s,  827.36/s)  LR: 1.000e-04  Data: 0.032 (0.412)
Train: 0 [  33/1251 (  3%)]  Loss:  7.031060 (7.0717)  Time: 0.855s, 1197.51/s  (1.226s,  834.95/s)  LR: 1.000e-04  Data: 0.034 (0.400)
Train: 0 [  34/1251 (  3%)]  Loss:  7.056376 (7.0713)  Time: 0.853s, 1200.45/s  (1.216s,  842.27/s)  LR: 1.000e-04  Data: 0.029 (0.390)
Train: 0 [  35/1251 (  3%)]  Loss:  7.066800 (7.0712)  Time: 0.864s, 1185.45/s  (1.206s,  849.10/s)  LR: 1.000e-04  Data: 0.041 (0.380)
Train: 0 [  36/1251 (  3%)]  Loss:  7.075976 (7.0713)  Time: 0.854s, 1199.33/s  (1.196s,  855.86/s)  LR: 1.000e-04  Data: 0.034 (0.371)
Train: 0 [  37/1251 (  3%)]  Loss:  7.067276 (7.0712)  Time: 0.859s, 1192.47/s  (1.188s,  862.26/s)  LR: 1.000e-04  Data: 0.038 (0.362)
Train: 0 [  38/1251 (  3%)]  Loss:  7.053072 (7.0707)  Time: 0.858s, 1193.50/s  (1.179s,  868.44/s)  LR: 1.000e-04  Data: 0.035 (0.354)
Train: 0 [  39/1251 (  3%)]  Loss:  7.058607 (7.0704)  Time: 0.857s, 1194.62/s  (1.171s,  874.41/s)  LR: 1.000e-04  Data: 0.036 (0.346)
Train: 0 [  40/1251 (  3%)]  Loss:  7.083585 (7.0707)  Time: 0.856s, 1196.39/s  (1.163s,  880.19/s)  LR: 1.000e-04  Data: 0.032 (0.338)
Train: 0 [  41/1251 (  3%)]  Loss:  7.061471 (7.0705)  Time: 0.854s, 1198.74/s  (1.156s,  885.79/s)  LR: 1.000e-04  Data: 0.031 (0.331)
Train: 0 [  42/1251 (  3%)]  Loss:  7.065951 (7.0704)  Time: 0.855s, 1196.98/s  (1.149s,  891.18/s)  LR: 1.000e-04  Data: 0.030 (0.324)
Train: 0 [  43/1251 (  3%)]  Loss:  7.061265 (7.0702)  Time: 0.856s, 1195.94/s  (1.142s,  896.37/s)  LR: 1.000e-04  Data: 0.032 (0.317)
Train: 0 [  44/1251 (  4%)]  Loss:  7.063822 (7.0701)  Time: 0.857s, 1194.67/s  (1.136s,  901.37/s)  LR: 1.000e-04  Data: 0.029 (0.311)
Train: 0 [  45/1251 (  4%)]  Loss:  7.076289 (7.0702)  Time: 0.857s, 1194.95/s  (1.130s,  906.21/s)  LR: 1.000e-04  Data: 0.033 (0.305)
Train: 0 [  46/1251 (  4%)]  Loss:  7.058413 (7.0700)  Time: 0.856s, 1196.92/s  (1.124s,  910.92/s)  LR: 1.000e-04  Data: 0.032 (0.299)
Train: 0 [  47/1251 (  4%)]  Loss:  7.052258 (7.0696)  Time: 0.861s, 1189.77/s  (1.119s,  915.39/s)  LR: 1.000e-04  Data: 0.033 (0.293)
Train: 0 [  48/1251 (  4%)]  Loss:  7.099480 (7.0702)  Time: 0.857s, 1195.17/s  (1.113s,  919.78/s)  LR: 1.000e-04  Data: 0.034 (0.288)
Train: 0 [  49/1251 (  4%)]  Loss:  7.037485 (7.0695)  Time: 0.857s, 1195.43/s  (1.108s,  924.05/s)  LR: 1.000e-04  Data: 0.034 (0.283)
Train: 0 [  50/1251 (  4%)]  Loss:  7.061627 (7.0694)  Time: 0.857s, 1194.23/s  (1.103s,  928.16/s)  LR: 1.000e-04  Data: 0.030 (0.278)
Train: 0 [  51/1251 (  4%)]  Loss:  7.068274 (7.0694)  Time: 0.857s, 1194.80/s  (1.099s,  932.16/s)  LR: 1.000e-04  Data: 0.032 (0.273)
Train: 0 [  52/1251 (  4%)]  Loss:  7.059595 (7.0692)  Time: 0.858s, 1194.12/s  (1.094s,  936.04/s)  LR: 1.000e-04  Data: 0.030 (0.269)
Train: 0 [  53/1251 (  4%)]  Loss:  7.087436 (7.0695)  Time: 0.856s, 1195.95/s  (1.090s,  939.82/s)  LR: 1.000e-04  Data: 0.034 (0.264)
Train: 0 [  54/1251 (  4%)]  Loss:  7.053214 (7.0692)  Time: 0.856s, 1196.14/s  (1.085s,  943.50/s)  LR: 1.000e-04  Data: 0.033 (0.260)
Train: 0 [  55/1251 (  4%)]  Loss:  7.090093 (7.0696)  Time: 0.856s, 1196.42/s  (1.081s,  947.07/s)  LR: 1.000e-04  Data: 0.034 (0.256)
Train: 0 [  56/1251 (  4%)]  Loss:  7.056655 (7.0694)  Time: 0.859s, 1192.08/s  (1.077s,  950.50/s)  LR: 1.000e-04  Data: 0.031 (0.252)
Train: 0 [  57/1251 (  5%)]  Loss:  7.058782 (7.0692)  Time: 0.857s, 1194.83/s  (1.074s,  953.86/s)  LR: 1.000e-04  Data: 0.032 (0.248)
Train: 0 [  58/1251 (  5%)]  Loss:  7.046130 (7.0688)  Time: 0.871s, 1175.55/s  (1.070s,  956.92/s)  LR: 1.000e-04  Data: 0.049 (0.245)
Train: 0 [  59/1251 (  5%)]  Loss:  7.063385 (7.0687)  Time: 0.856s, 1196.89/s  (1.067s,  960.13/s)  LR: 1.000e-04  Data: 0.034 (0.241)
Train: 0 [  60/1251 (  5%)]  Loss:  7.063017 (7.0686)  Time: 0.856s, 1196.81/s  (1.063s,  963.25/s)  LR: 1.000e-04  Data: 0.029 (0.238)
Train: 0 [  61/1251 (  5%)]  Loss:  7.066967 (7.0686)  Time: 0.855s, 1197.12/s  (1.060s,  966.30/s)  LR: 1.000e-04  Data: 0.031 (0.235)
Train: 0 [  62/1251 (  5%)]  Loss:  7.071018 (7.0686)  Time: 0.858s, 1194.04/s  (1.057s,  969.23/s)  LR: 1.000e-04  Data: 0.032 (0.231)
Train: 0 [  63/1251 (  5%)]  Loss:  7.080926 (7.0688)  Time: 0.857s, 1195.34/s  (1.053s,  972.10/s)  LR: 1.000e-04  Data: 0.034 (0.228)
Train: 0 [  64/1251 (  5%)]  Loss:  7.088578 (7.0691)  Time: 0.856s, 1196.86/s  (1.050s,  974.92/s)  LR: 1.000e-04  Data: 0.036 (0.225)
Train: 0 [  65/1251 (  5%)]  Loss:  7.050634 (7.0688)  Time: 0.856s, 1196.87/s  (1.047s,  977.67/s)  LR: 1.000e-04  Data: 0.030 (0.222)
Train: 0 [  66/1251 (  5%)]  Loss:  7.070393 (7.0689)  Time: 0.856s, 1196.06/s  (1.045s,  980.34/s)  LR: 1.000e-04  Data: 0.032 (0.220)
Train: 0 [  67/1251 (  5%)]  Loss:  7.037153 (7.0684)  Time: 0.858s, 1193.01/s  (1.042s,  982.92/s)  LR: 1.000e-04  Data: 0.036 (0.217)
Train: 0 [  68/1251 (  5%)]  Loss:  7.077674 (7.0685)  Time: 0.854s, 1198.44/s  (1.039s,  985.48/s)  LR: 1.000e-04  Data: 0.031 (0.214)
Train: 0 [  69/1251 (  6%)]  Loss:  7.047565 (7.0682)  Time: 0.854s, 1198.60/s  (1.036s,  987.99/s)  LR: 1.000e-04  Data: 0.032 (0.212)
Train: 0 [  70/1251 (  6%)]  Loss:  7.057567 (7.0681)  Time: 0.856s, 1196.35/s  (1.034s,  990.42/s)  LR: 1.000e-04  Data: 0.033 (0.209)
Train: 0 [  71/1251 (  6%)]  Loss:  7.068780 (7.0681)  Time: 0.857s, 1195.36/s  (1.031s,  992.79/s)  LR: 1.000e-04  Data: 0.034 (0.207)
Train: 0 [  72/1251 (  6%)]  Loss:  7.049481 (7.0678)  Time: 0.857s, 1195.50/s  (1.029s,  995.10/s)  LR: 1.000e-04  Data: 0.036 (0.204)
Train: 0 [  73/1251 (  6%)]  Loss:  7.052855 (7.0676)  Time: 0.856s, 1196.96/s  (1.027s,  997.37/s)  LR: 1.000e-04  Data: 0.034 (0.202)
Train: 0 [  74/1251 (  6%)]  Loss:  7.053963 (7.0675)  Time: 0.856s, 1196.61/s  (1.024s,  999.59/s)  LR: 1.000e-04  Data: 0.031 (0.200)
Train: 0 [  75/1251 (  6%)]  Loss:  7.046283 (7.0672)  Time: 0.860s, 1190.01/s  (1.022s, 1001.70/s)  LR: 1.000e-04  Data: 0.037 (0.198)
Train: 0 [  76/1251 (  6%)]  Loss:  7.061117 (7.0671)  Time: 0.855s, 1198.05/s  (1.020s, 1003.84/s)  LR: 1.000e-04  Data: 0.033 (0.195)
Train: 0 [  77/1251 (  6%)]  Loss:  7.066966 (7.0671)  Time: 0.853s, 1200.41/s  (1.018s, 1005.95/s)  LR: 1.000e-04  Data: 0.032 (0.193)
Train: 0 [  78/1251 (  6%)]  Loss:  7.080099 (7.0673)  Time: 0.869s, 1178.16/s  (1.016s, 1007.81/s)  LR: 1.000e-04  Data: 0.032 (0.191)
Train: 0 [  79/1251 (  6%)]  Loss:  7.061601 (7.0672)  Time: 0.857s, 1194.67/s  (1.014s, 1009.79/s)  LR: 1.000e-04  Data: 0.032 (0.189)
Train: 0 [  80/1251 (  6%)]  Loss:  7.045609 (7.0669)  Time: 0.856s, 1195.67/s  (1.012s, 1011.73/s)  LR: 1.000e-04  Data: 0.035 (0.187)
Train: 0 [  81/1251 (  6%)]  Loss:  7.048256 (7.0667)  Time: 0.855s, 1197.83/s  (1.010s, 1013.65/s)  LR: 1.000e-04  Data: 0.035 (0.186)
Train: 0 [  82/1251 (  7%)]  Loss:  7.088354 (7.0670)  Time: 0.857s, 1194.68/s  (1.008s, 1015.50/s)  LR: 1.000e-04  Data: 0.035 (0.184)
Train: 0 [  83/1251 (  7%)]  Loss:  7.072888 (7.0670)  Time: 0.853s, 1199.77/s  (1.007s, 1017.36/s)  LR: 1.000e-04  Data: 0.032 (0.182)
Train: 0 [  84/1251 (  7%)]  Loss:  7.052944 (7.0669)  Time: 0.855s, 1197.85/s  (1.005s, 1019.17/s)  LR: 1.000e-04  Data: 0.034 (0.180)
Train: 0 [  85/1251 (  7%)]  Loss:  7.060890 (7.0668)  Time: 0.857s, 1195.55/s  (1.003s, 1020.92/s)  LR: 1.000e-04  Data: 0.037 (0.179)
Train: 0 [  86/1251 (  7%)]  Loss:  7.079667 (7.0669)  Time: 0.856s, 1196.93/s  (1.001s, 1022.65/s)  LR: 1.000e-04  Data: 0.031 (0.177)
Train: 0 [  87/1251 (  7%)]  Loss:  7.063510 (7.0669)  Time: 0.857s, 1194.95/s  (1.000s, 1024.33/s)  LR: 1.000e-04  Data: 0.033 (0.175)
Train: 0 [  88/1251 (  7%)]  Loss:  7.056036 (7.0668)  Time: 0.854s, 1199.01/s  (0.998s, 1026.01/s)  LR: 1.000e-04  Data: 0.032 (0.174)
Train: 0 [  89/1251 (  7%)]  Loss:  7.070159 (7.0668)  Time: 0.853s, 1199.82/s  (0.996s, 1027.66/s)  LR: 1.000e-04  Data: 0.033 (0.172)
Train: 0 [  90/1251 (  7%)]  Loss:  7.092608 (7.0671)  Time: 0.856s, 1196.92/s  (0.995s, 1029.26/s)  LR: 1.000e-04  Data: 0.034 (0.170)
Train: 0 [  91/1251 (  7%)]  Loss:  7.048456 (7.0669)  Time: 0.856s, 1195.75/s  (0.993s, 1030.82/s)  LR: 1.000e-04  Data: 0.034 (0.169)
Train: 0 [  92/1251 (  7%)]  Loss:  7.047908 (7.0667)  Time: 0.855s, 1197.62/s  (0.992s, 1032.37/s)  LR: 1.000e-04  Data: 0.033 (0.168)
Train: 0 [  93/1251 (  7%)]  Loss:  7.054535 (7.0666)  Time: 0.855s, 1197.20/s  (0.990s, 1033.88/s)  LR: 1.000e-04  Data: 0.033 (0.166)
Train: 0 [  94/1251 (  8%)]  Loss:  7.075508 (7.0667)  Time: 0.857s, 1195.30/s  (0.989s, 1035.35/s)  LR: 1.000e-04  Data: 0.032 (0.165)
Train: 0 [  95/1251 (  8%)]  Loss:  7.057827 (7.0666)  Time: 0.854s, 1199.56/s  (0.988s, 1036.83/s)  LR: 1.000e-04  Data: 0.033 (0.163)
Train: 0 [  96/1251 (  8%)]  Loss:  7.045257 (7.0663)  Time: 1.334s,  767.67/s  (0.991s, 1033.10/s)  LR: 1.000e-04  Data: 0.031 (0.162)
Train: 0 [  97/1251 (  8%)]  Loss:  7.048449 (7.0662)  Time: 7.111s,  144.00/s  (1.054s,  971.87/s)  LR: 1.000e-04  Data: 0.031 (0.161)
Train: 0 [  98/1251 (  8%)]  Loss:  7.039270 (7.0659)  Time: 0.854s, 1198.82/s  (1.052s,  973.73/s)  LR: 1.000e-04  Data: 0.033 (0.159)
Train: 0 [  99/1251 (  8%)]  Loss:  7.056643 (7.0658)  Time: 12.568s,   81.48/s  (1.167s,  877.62/s)  LR: 1.000e-04  Data: 11.745 (0.275)
