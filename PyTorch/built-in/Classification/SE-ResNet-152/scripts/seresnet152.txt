--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
/root/miniconda3/envs/torch_env_py310/lib/python3.10/site-packages/torch/distributed/launch.py:208: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  main()
W0214 08:47:02.850000 140017238701888 torch/distributed/run.py:779] 
W0214 08:47:02.850000 140017238701888 torch/distributed/run.py:779] *****************************************
W0214 08:47:02.850000 140017238701888 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0214 08:47:02.850000 140017238701888 torch/distributed/run.py:779] *****************************************
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
--------------+----------------------------------------------
 Host IP      | 20.21.22.4
 PyTorch      | 2.4.0a0+git4451b0e
 Torch-SDAA   | 2.0.0
--------------+----------------------------------------------
 SDAA Driver  | 2.0.0 (N/A)
 SDAA Runtime | 2.0.0 (/opt/tecoai/lib64/libsdaart.so)
 SDPTI        | 1.3.1 (/opt/tecoai/lib64/libsdpti.so)
 TecoDNN      | 2.0.0 (/opt/tecoai/lib64/libtecodnn.so)
 TecoBLAS     | 2.0.0 (/opt/tecoai/lib64/libtecoblas.so)
 CustomDNN    | 1.22.0 (/opt/tecoai/lib64/libtecodnn_ext.so)
 TecoRAND     | 1.8.0 (/opt/tecoai/lib64/libtecorand.so)
 TCCL         | 1.21.0 (/opt/tecoai/lib64/libtccl.so)
--------------+----------------------------------------------
Training in distributed mode with multiple processes, 1 GPU per process. Process 0, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 1, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 2, total 4.
Training in distributed mode with multiple processes, 1 GPU per process. Process 3, total 4.
Model seresnet152 created, param count: 66821848
Data processing configuration for current model + dataset:
	input_size: (3, 224, 224)
	interpolation: bilinear
	mean: (0.485, 0.456, 0.406)
	std: (0.229, 0.224, 0.225)
	crop_pct: 0.875
NVIDIA APEX not installed. AMP off.
Using torch DistributedDataParallel. Install NVIDIA Apex for Apex DDP.
Scheduled epochs: 150
Train: 0 [   0/10009 (  0%)]  Loss:  6.994201 (6.9942)  Time: 9.289s,   13.78/s  (9.289s,   13.78/s)  LR: 1.000e-04  Data: 6.791 (6.791)
Train: 0 [   1/10009 (  0%)]  Loss:  6.975311 (6.9848)  Time: 0.609s,  210.26/s  (4.949s,   25.86/s)  LR: 1.000e-04  Data: 0.006 (3.399)
Train: 0 [   2/10009 (  0%)]  Loss:  6.953767 (6.9744)  Time: 0.604s,  212.08/s  (3.501s,   36.57/s)  LR: 1.000e-04  Data: 0.005 (2.267)
Train: 0 [   3/10009 (  0%)]  Loss:  6.992209 (6.9789)  Time: 4.978s,   25.71/s  (3.870s,   33.08/s)  LR: 1.000e-04  Data: 4.380 (2.796)
Train: 0 [   4/10009 (  0%)]  Loss:  6.938873 (6.9709)  Time: 0.605s,  211.40/s  (3.217s,   39.79/s)  LR: 1.000e-04  Data: 0.004 (2.237)
Train: 0 [   5/10009 (  0%)]  Loss:  7.028868 (6.9805)  Time: 0.613s,  208.80/s  (2.783s,   45.99/s)  LR: 1.000e-04  Data: 0.005 (1.865)
Train: 0 [   6/10009 (  0%)]  Loss:  6.958966 (6.9775)  Time: 0.603s,  212.24/s  (2.472s,   51.79/s)  LR: 1.000e-04  Data: 0.005 (1.600)
Train: 0 [   7/10009 (  0%)]  Loss:  6.991691 (6.9792)  Time: 1.623s,   78.87/s  (2.365s,   54.11/s)  LR: 1.000e-04  Data: 1.024 (1.528)
Train: 0 [   8/10009 (  0%)]  Loss:  7.032728 (6.9852)  Time: 0.605s,  211.74/s  (2.170s,   58.99/s)  LR: 1.000e-04  Data: 0.006 (1.359)
Train: 0 [   9/10009 (  0%)]  Loss:  6.921359 (6.9788)  Time: 0.604s,  212.09/s  (2.013s,   63.58/s)  LR: 1.000e-04  Data: 0.004 (1.223)
Train: 0 [  10/10009 (  0%)]  Loss:  7.047087 (6.9850)  Time: 0.603s,  212.18/s  (1.885s,   67.90/s)  LR: 1.000e-04  Data: 0.004 (1.112)
Train: 0 [  11/10009 (  0%)]  Loss:  6.970831 (6.9838)  Time: 0.884s,  144.80/s  (1.802s,   71.05/s)  LR: 1.000e-04  Data: 0.285 (1.043)
Train: 0 [  12/10009 (  0%)]  Loss:  6.983435 (6.9838)  Time: 0.604s,  211.95/s  (1.709s,   74.88/s)  LR: 1.000e-04  Data: 0.005 (0.963)
Train: 0 [  13/10009 (  0%)]  Loss:  7.008841 (6.9856)  Time: 0.604s,  211.88/s  (1.631s,   78.50/s)  LR: 1.000e-04  Data: 0.004 (0.895)
Train: 0 [  14/10009 (  0%)]  Loss:  6.934830 (6.9822)  Time: 0.605s,  211.61/s  (1.562s,   81.94/s)  LR: 1.000e-04  Data: 0.006 (0.836)
Train: 0 [  15/10009 (  0%)]  Loss:  6.980458 (6.9821)  Time: 0.605s,  211.73/s  (1.502s,   85.20/s)  LR: 1.000e-04  Data: 0.006 (0.784)
Train: 0 [  16/10009 (  0%)]  Loss:  6.954387 (6.9805)  Time: 0.605s,  211.65/s  (1.449s,   88.31/s)  LR: 1.000e-04  Data: 0.007 (0.738)
Train: 0 [  17/10009 (  0%)]  Loss:  7.001970 (6.9817)  Time: 0.603s,  212.34/s  (1.402s,   91.27/s)  LR: 1.000e-04  Data: 0.004 (0.697)
Train: 0 [  18/10009 (  0%)]  Loss:  6.998705 (6.9826)  Time: 0.603s,  212.37/s  (1.360s,   94.09/s)  LR: 1.000e-04  Data: 0.004 (0.661)
Train: 0 [  19/10009 (  0%)]  Loss:  6.970626 (6.9820)  Time: 0.604s,  211.88/s  (1.323s,   96.78/s)  LR: 1.000e-04  Data: 0.006 (0.628)
Train: 0 [  20/10009 (  0%)]  Loss:  7.001644 (6.9829)  Time: 1.029s,  124.41/s  (1.309s,   97.82/s)  LR: 1.000e-04  Data: 0.430 (0.619)
Train: 0 [  21/10009 (  0%)]  Loss:  6.930062 (6.9805)  Time: 0.605s,  211.50/s  (1.277s,  100.27/s)  LR: 1.000e-04  Data: 0.007 (0.591)
Train: 0 [  22/10009 (  0%)]  Loss:  6.999693 (6.9813)  Time: 0.603s,  212.21/s  (1.247s,  102.62/s)  LR: 1.000e-04  Data: 0.005 (0.565)
Train: 0 [  23/10009 (  0%)]  Loss:  6.992476 (6.9818)  Time: 1.332s,   96.12/s  (1.251s,  102.33/s)  LR: 1.000e-04  Data: 0.733 (0.572)
Train: 0 [  24/10009 (  0%)]  Loss:  6.972566 (6.9814)  Time: 0.794s,  161.19/s  (1.233s,  103.85/s)  LR: 1.000e-04  Data: 0.196 (0.557)
Train: 0 [  25/10009 (  0%)]  Loss:  7.022819 (6.9830)  Time: 0.603s,  212.14/s  (1.208s,  105.93/s)  LR: 1.000e-04  Data: 0.004 (0.536)
Train: 0 [  26/10009 (  0%)]  Loss:  6.972364 (6.9826)  Time: 0.603s,  212.32/s  (1.186s,  107.93/s)  LR: 1.000e-04  Data: 0.004 (0.516)
Train: 0 [  27/10009 (  0%)]  Loss:  6.996903 (6.9831)  Time: 0.604s,  212.03/s  (1.165s,  109.86/s)  LR: 1.000e-04  Data: 0.006 (0.498)
Train: 0 [  28/10009 (  0%)]  Loss:  6.972507 (6.9828)  Time: 1.313s,   97.50/s  (1.170s,  109.38/s)  LR: 1.000e-04  Data: 0.715 (0.506)
Train: 0 [  29/10009 (  0%)]  Loss:  6.948620 (6.9816)  Time: 0.604s,  212.05/s  (1.151s,  111.17/s)  LR: 1.000e-04  Data: 0.004 (0.489)
Train: 0 [  30/10009 (  0%)]  Loss:  6.946944 (6.9805)  Time: 0.603s,  212.11/s  (1.134s,  112.91/s)  LR: 1.000e-04  Data: 0.004 (0.473)
Train: 0 [  31/10009 (  0%)]  Loss:  7.024319 (6.9819)  Time: 1.641s,   77.98/s  (1.150s,  111.35/s)  LR: 1.000e-04  Data: 1.043 (0.491)
Train: 0 [  32/10009 (  0%)]  Loss:  7.013294 (6.9828)  Time: 0.607s,  210.98/s  (1.133s,  112.97/s)  LR: 1.000e-04  Data: 0.005 (0.476)
Train: 0 [  33/10009 (  0%)]  Loss:  6.976099 (6.9826)  Time: 0.603s,  212.26/s  (1.118s,  114.54/s)  LR: 1.000e-04  Data: 0.004 (0.462)
Train: 0 [  34/10009 (  0%)]  Loss:  7.044958 (6.9844)  Time: 0.604s,  212.00/s  (1.103s,  116.07/s)  LR: 1.000e-04  Data: 0.005 (0.449)
Train: 0 [  35/10009 (  0%)]  Loss:  7.060662 (6.9865)  Time: 1.581s,   80.96/s  (1.116s,  114.68/s)  LR: 1.000e-04  Data: 0.957 (0.463)
Train: 0 [  36/10009 (  0%)]  Loss:  6.968603 (6.9860)  Time: 2.091s,   61.21/s  (1.142s,  112.04/s)  LR: 1.000e-04  Data: 1.493 (0.491)
Train: 0 [  37/10009 (  0%)]  Loss:  7.000650 (6.9864)  Time: 0.604s,  211.81/s  (1.128s,  113.45/s)  LR: 1.000e-04  Data: 0.004 (0.478)
Train: 0 [  38/10009 (  0%)]  Loss:  6.983954 (6.9864)  Time: 0.603s,  212.26/s  (1.115s,  114.82/s)  LR: 1.000e-04  Data: 0.004 (0.466)
Train: 0 [  39/10009 (  0%)]  Loss:  6.991831 (6.9865)  Time: 1.991s,   64.29/s  (1.137s,  112.60/s)  LR: 1.000e-04  Data: 1.393 (0.489)
Train: 0 [  40/10009 (  0%)]  Loss:  7.057553 (6.9882)  Time: 0.603s,  212.28/s  (1.124s,  113.91/s)  LR: 1.000e-04  Data: 0.005 (0.478)
Train: 0 [  41/10009 (  0%)]  Loss:  6.994756 (6.9884)  Time: 0.605s,  211.74/s  (1.111s,  115.17/s)  LR: 1.000e-04  Data: 0.004 (0.466)
Train: 0 [  42/10009 (  0%)]  Loss:  7.003279 (6.9887)  Time: 0.603s,  212.26/s  (1.100s,  116.41/s)  LR: 1.000e-04  Data: 0.005 (0.456)
Train: 0 [  43/10009 (  0%)]  Loss:  6.968029 (6.9883)  Time: 1.096s,  116.80/s  (1.099s,  116.42/s)  LR: 1.000e-04  Data: 0.497 (0.457)
Train: 0 [  44/10009 (  0%)]  Loss:  6.970611 (6.9879)  Time: 0.732s,  174.91/s  (1.091s,  117.29/s)  LR: 1.000e-04  Data: 0.134 (0.449)
Train: 0 [  45/10009 (  0%)]  Loss:  7.011484 (6.9884)  Time: 0.604s,  211.90/s  (1.081s,  118.44/s)  LR: 1.000e-04  Data: 0.004 (0.440)
Train: 0 [  46/10009 (  0%)]  Loss:  7.024787 (6.9892)  Time: 0.604s,  211.89/s  (1.071s,  119.56/s)  LR: 1.000e-04  Data: 0.005 (0.430)
Train: 0 [  47/10009 (  0%)]  Loss:  6.981218 (6.9890)  Time: 1.348s,   94.93/s  (1.076s,  118.92/s)  LR: 1.000e-04  Data: 0.750 (0.437)
Train: 0 [  48/10009 (  0%)]  Loss:  6.993196 (6.9891)  Time: 0.664s,  192.89/s  (1.068s,  119.86/s)  LR: 1.000e-04  Data: 0.065 (0.430)
Train: 0 [  49/10009 (  0%)]  Loss:  6.996599 (6.9892)  Time: 0.604s,  211.82/s  (1.059s,  120.91/s)  LR: 1.000e-04  Data: 0.005 (0.421)
Train: 0 [  50/10009 (  0%)]  Loss:  7.044026 (6.9903)  Time: 0.603s,  212.31/s  (1.050s,  121.94/s)  LR: 1.000e-04  Data: 0.004 (0.413)
Train: 0 [  51/10009 (  1%)]  Loss:  7.051546 (6.9915)  Time: 1.862s,   68.74/s  (1.065s,  120.15/s)  LR: 1.000e-04  Data: 0.884 (0.422)
Train: 0 [  52/10009 (  1%)]  Loss:  7.010582 (6.9918)  Time: 0.604s,  211.95/s  (1.057s,  121.14/s)  LR: 1.000e-04  Data: 0.005 (0.414)
Train: 0 [  53/10009 (  1%)]  Loss:  6.963827 (6.9913)  Time: 0.604s,  211.93/s  (1.048s,  122.11/s)  LR: 1.000e-04  Data: 0.006 (0.407)
Train: 0 [  54/10009 (  1%)]  Loss:  6.955971 (6.9907)  Time: 0.603s,  212.24/s  (1.040s,  123.06/s)  LR: 1.000e-04  Data: 0.005 (0.399)
Train: 0 [  55/10009 (  1%)]  Loss:  7.007168 (6.9910)  Time: 1.488s,   86.05/s  (1.048s,  122.12/s)  LR: 1.000e-04  Data: 0.414 (0.399)
Train: 0 [  56/10009 (  1%)]  Loss:  6.987189 (6.9909)  Time: 1.929s,   66.34/s  (1.064s,  120.35/s)  LR: 1.000e-04  Data: 1.331 (0.416)
Train: 0 [  57/10009 (  1%)]  Loss:  7.028512 (6.9916)  Time: 0.607s,  210.73/s  (1.056s,  121.24/s)  LR: 1.000e-04  Data: 0.005 (0.409)
Train: 0 [  58/10009 (  1%)]  Loss:  6.985750 (6.9915)  Time: 0.603s,  212.43/s  (1.048s,  122.13/s)  LR: 1.000e-04  Data: 0.004 (0.402)
Train: 0 [  59/10009 (  1%)]  Loss:  7.015665 (6.9919)  Time: 1.706s,   75.03/s  (1.059s,  120.87/s)  LR: 1.000e-04  Data: 0.005 (0.395)
Train: 0 [  60/10009 (  1%)]  Loss:  7.008441 (6.9921)  Time: 0.616s,  207.92/s  (1.052s,  121.70/s)  LR: 1.000e-04  Data: 0.017 (0.389)
Train: 0 [  61/10009 (  1%)]  Loss:  6.988542 (6.9921)  Time: 0.612s,  209.04/s  (1.045s,  122.53/s)  LR: 1.000e-04  Data: 0.005 (0.383)
Train: 0 [  62/10009 (  1%)]  Loss:  7.000698 (6.9922)  Time: 0.604s,  212.08/s  (1.038s,  123.36/s)  LR: 1.000e-04  Data: 0.005 (0.377)
Train: 0 [  63/10009 (  1%)]  Loss:  6.986802 (6.9921)  Time: 1.351s,   94.75/s  (1.043s,  122.78/s)  LR: 1.000e-04  Data: 0.005 (0.371)
Train: 0 [  64/10009 (  1%)]  Loss:  6.978390 (6.9919)  Time: 0.605s,  211.55/s  (1.036s,  123.57/s)  LR: 1.000e-04  Data: 0.007 (0.365)
Train: 0 [  65/10009 (  1%)]  Loss:  6.974560 (6.9917)  Time: 0.604s,  211.99/s  (1.029s,  124.36/s)  LR: 1.000e-04  Data: 0.005 (0.360)
Train: 0 [  66/10009 (  1%)]  Loss:  6.989121 (6.9916)  Time: 0.603s,  212.44/s  (1.023s,  125.13/s)  LR: 1.000e-04  Data: 0.004 (0.355)
Train: 0 [  67/10009 (  1%)]  Loss:  7.019374 (6.9920)  Time: 1.665s,   76.87/s  (1.032s,  123.99/s)  LR: 1.000e-04  Data: 0.367 (0.355)
Train: 0 [  68/10009 (  1%)]  Loss:  6.972684 (6.9917)  Time: 2.052s,   62.38/s  (1.047s,  122.24/s)  LR: 1.000e-04  Data: 1.454 (0.371)
Train: 0 [  69/10009 (  1%)]  Loss:  7.011304 (6.9920)  Time: 0.604s,  212.02/s  (1.041s,  122.98/s)  LR: 1.000e-04  Data: 0.004 (0.366)
Train: 0 [  70/10009 (  1%)]  Loss:  6.984281 (6.9919)  Time: 0.603s,  212.37/s  (1.035s,  123.72/s)  LR: 1.000e-04  Data: 0.005 (0.360)
Train: 0 [  71/10009 (  1%)]  Loss:  6.925650 (6.9910)  Time: 0.604s,  211.83/s  (1.029s,  124.44/s)  LR: 1.000e-04  Data: 0.004 (0.356)
Train: 0 [  72/10009 (  1%)]  Loss:  6.936256 (6.9902)  Time: 1.852s,   69.10/s  (1.040s,  123.09/s)  LR: 1.000e-04  Data: 1.254 (0.368)
Train: 0 [  73/10009 (  1%)]  Loss:  6.989893 (6.9902)  Time: 0.604s,  212.05/s  (1.034s,  123.79/s)  LR: 1.000e-04  Data: 0.004 (0.363)
Train: 0 [  74/10009 (  1%)]  Loss:  6.972189 (6.9900)  Time: 0.604s,  212.07/s  (1.028s,  124.48/s)  LR: 1.000e-04  Data: 0.005 (0.358)
Train: 0 [  75/10009 (  1%)]  Loss:  6.979907 (6.9899)  Time: 2.078s,   61.59/s  (1.042s,  122.83/s)  LR: 1.000e-04  Data: 0.005 (0.354)
Train: 0 [  76/10009 (  1%)]  Loss:  6.981817 (6.9898)  Time: 0.604s,  211.92/s  (1.036s,  123.50/s)  LR: 1.000e-04  Data: 0.006 (0.349)
Train: 0 [  77/10009 (  1%)]  Loss:  6.958005 (6.9894)  Time: 0.604s,  211.84/s  (1.031s,  124.17/s)  LR: 1.000e-04  Data: 0.005 (0.345)
Train: 0 [  78/10009 (  1%)]  Loss:  6.942458 (6.9888)  Time: 0.603s,  212.24/s  (1.025s,  124.82/s)  LR: 1.000e-04  Data: 0.005 (0.340)
Train: 0 [  79/10009 (  1%)]  Loss:  6.925301 (6.9880)  Time: 1.568s,   81.61/s  (1.032s,  124.00/s)  LR: 1.000e-04  Data: 0.005 (0.336)
Train: 0 [  80/10009 (  1%)]  Loss:  6.952414 (6.9875)  Time: 2.118s,   60.44/s  (1.046s,  122.41/s)  LR: 1.000e-04  Data: 1.520 (0.351)
Train: 0 [  81/10009 (  1%)]  Loss:  6.965345 (6.9873)  Time: 0.603s,  212.25/s  (1.040s,  123.05/s)  LR: 1.000e-04  Data: 0.005 (0.346)
Train: 0 [  82/10009 (  1%)]  Loss:  6.959168 (6.9869)  Time: 0.603s,  212.29/s  (1.035s,  123.67/s)  LR: 1.000e-04  Data: 0.004 (0.342)
Train: 0 [  83/10009 (  1%)]  Loss:  7.016810 (6.9873)  Time: 1.351s,   94.77/s  (1.039s,  123.23/s)  LR: 1.000e-04  Data: 0.006 (0.338)
Train: 0 [  84/10009 (  1%)]  Loss:  6.951619 (6.9869)  Time: 0.616s,  207.81/s  (1.034s,  123.82/s)  LR: 1.000e-04  Data: 0.006 (0.334)
Train: 0 [  85/10009 (  1%)]  Loss:  7.022075 (6.9873)  Time: 0.602s,  212.45/s  (1.029s,  124.42/s)  LR: 1.000e-04  Data: 0.004 (0.331)
Train: 0 [  86/10009 (  1%)]  Loss:  7.020178 (6.9876)  Time: 0.604s,  212.09/s  (1.024s,  125.02/s)  LR: 1.000e-04  Data: 0.005 (0.327)
Train: 0 [  87/10009 (  1%)]  Loss:  7.039630 (6.9882)  Time: 1.678s,   76.30/s  (1.031s,  124.12/s)  LR: 1.000e-04  Data: 0.005 (0.323)
Train: 0 [  88/10009 (  1%)]  Loss:  6.932369 (6.9876)  Time: 0.860s,  148.78/s  (1.029s,  124.35/s)  LR: 1.000e-04  Data: 0.262 (0.323)
Train: 0 [  89/10009 (  1%)]  Loss:  7.018646 (6.9880)  Time: 0.605s,  211.69/s  (1.025s,  124.92/s)  LR: 1.000e-04  Data: 0.004 (0.319)
Train: 0 [  90/10009 (  1%)]  Loss:  6.930766 (6.9873)  Time: 0.604s,  211.75/s  (1.020s,  125.49/s)  LR: 1.000e-04  Data: 0.005 (0.316)
Train: 0 [  91/10009 (  1%)]  Loss:  6.982947 (6.9873)  Time: 1.304s,   98.15/s  (1.023s,  125.11/s)  LR: 1.000e-04  Data: 0.005 (0.312)
Train: 0 [  92/10009 (  1%)]  Loss:  6.982576 (6.9872)  Time: 2.249s,   56.90/s  (1.036s,  123.52/s)  LR: 1.000e-04  Data: 1.651 (0.327)
Train: 0 [  93/10009 (  1%)]  Loss:  6.991971 (6.9873)  Time: 0.603s,  212.11/s  (1.032s,  124.07/s)  LR: 1.000e-04  Data: 0.004 (0.323)
Train: 0 [  94/10009 (  1%)]  Loss:  6.977554 (6.9872)  Time: 0.605s,  211.71/s  (1.027s,  124.61/s)  LR: 1.000e-04  Data: 0.005 (0.320)
Train: 0 [  95/10009 (  1%)]  Loss:  6.990201 (6.9872)  Time: 0.604s,  211.82/s  (1.023s,  125.15/s)  LR: 1.000e-04  Data: 0.006 (0.317)
Train: 0 [  96/10009 (  1%)]  Loss:  6.960749 (6.9869)  Time: 1.695s,   75.50/s  (1.030s,  124.30/s)  LR: 1.000e-04  Data: 1.096 (0.325)
Train: 0 [  97/10009 (  1%)]  Loss:  6.925008 (6.9863)  Time: 0.604s,  211.81/s  (1.025s,  124.83/s)  LR: 1.000e-04  Data: 0.005 (0.321)
Train: 0 [  98/10009 (  1%)]  Loss:  6.909861 (6.9855)  Time: 0.603s,  212.15/s  (1.021s,  125.35/s)  LR: 1.000e-04  Data: 0.004 (0.318)
Train: 0 [  99/10009 (  1%)]  Loss:  6.972027 (6.9854)  Time: 0.603s,  212.22/s  (1.017s,  125.87/s)  LR: 1.000e-04  Data: 0.005 (0.315)
