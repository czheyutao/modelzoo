Setting Arguments.. :  Namespace(batch_size=4, conf_thresh=0.001, cuda=True, dataset='coco', debug=False, dist_url='env://', distributed=False, ema=True, eval_epoch=10, eval_first=False, find_unused_parameters=False, fp16=True, grad_accumulate=1, img_size=640, load_cache=False, max_epoch=1, min_box_size=8.0, mixup=None, model='yolov1', mosaic=None, multi_scale=True, nms_class_agnostic=False, nms_thresh=0.7, no_aug_epoch=20, no_multi_labels=False, num_workers=4, pretrained=None, resume=None, root='./dataset', save_folder='weights/', seed=42, sybn=False, tfboard=False, topk=1000, vis_aux_loss=False, vis_tgt=False, world_size=1, wp_epoch=1)
----------------------------------------------------------
LOCAL RANK:  -1
LOCAL_PROCESS_RANL:  -1
WORLD SIZE: 1
use cuda
==============================
Dataset Config: {'data_name': 'COCO', 'num_classes': 80, 'class_indexs': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90], 'class_names': ('background', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush')} 

==============================
Model: YOLOV1 ...
==============================
Transform: ssd-Style ...
Transform Config: {'aug_type': 'ssd', 'use_ablu': False, 'mosaic_prob': 0.0, 'mixup_prob': 0.0, 'mosaic_type': 'yolov5', 'mixup_type': 'yolov5', 'mixup_scale': [0.5, 1.5]} 

==============================
Build YOLOV1 ...
==============================
Model Configuration: 
 {'backbone': 'resnet18', 'pretrained': True, 'stride': 32, 'max_stride': 32, 'neck': 'sppf', 'neck_act': 'lrelu', 'neck_norm': 'BN', 'neck_depthwise': False, 'expand_ratio': 0.5, 'pooling_size': 5, 'head': 'decoupled_head', 'head_act': 'lrelu', 'head_norm': 'BN', 'num_cls_head': 2, 'num_reg_head': 2, 'head_depthwise': False, 'multi_scale': [0.5, 1.5], 'trans_type': 'ssd', 'loss_obj_weight': 1.0, 'loss_cls_weight': 1.0, 'loss_box_weight': 5.0, 'trainer_type': 'yolo'}
==============================
Neck: sppf
==============================
Head: Decoupled Head
==============================
GFLOPs : 37.89
Params : 21.32 M
loading annotations into memory...
Done (t=13.83s)
creating index...
index created!
==============================
use Mosaic Augmentation: 0.0
use Mixup Augmentation: 0.0
loading annotations into memory...
Done (t=1.89s)
creating index...
index created!
==============================
use Mosaic Augmentation: 0.0
use Mixup Augmentation: 0.0
==============================
Optimizer: adamw
--base lr: 6.25e-05
--momentum: None
--weight_decay: 0.05
==============================
Lr Scheduler: linear
Build ModelEMA ...
============== Second stage of Training ==============
 - Rebuild transforms ...
Saving state of the last Mosaic epoch-0.
Iter 0: Loss = 15.880165100097656
Epoch: [0 / 1]  [    0/29316]  eta: 10:19:09  lr: 0.000000  size: 960  grad_norm: nan  loss_obj: 6.1715 (6.1715)  loss_cls: 5.3396 (5.3396)  loss_box: 0.8738 (0.8738)  losses: 15.8802 (15.8802)  time: 1.2672  data: 0.5783  max mem: 1185
Iter 1: Loss = 14.908965110778809
Iter 2: Loss = 13.642522811889648
Iter 3: Loss = 16.796566009521484
Iter 4: Loss = 14.938735961914062
Iter 5: Loss = 14.6658296585083
Iter 6: Loss = 14.107046127319336
Iter 7: Loss = 13.828215599060059
Iter 8: Loss = 16.279884338378906
Iter 9: Loss = 14.444568634033203
Iter 10: Loss = 13.881232261657715
Epoch: [0 / 1]  [   10/29316]  eta: 1:32:10  lr: 0.000000  size: 896  grad_norm: 57.6  loss_obj: 5.1826 (5.4414)  loss_cls: 5.3866 (5.2446)  loss_box: 0.8528 (0.8332)  losses: 14.6658 (14.8522)  time: 0.1887  data: 0.0528  max mem: 1234
Iter 11: Loss = 14.104101181030273
Iter 12: Loss = 12.655430793762207
Iter 13: Loss = 14.596073150634766
Iter 14: Loss = 14.326417922973633
Iter 15: Loss = 13.171818733215332
Iter 16: Loss = 14.87158203125
Iter 17: Loss = 14.598150253295898
Iter 18: Loss = 14.314703941345215
Iter 19: Loss = 14.077291488647461
Iter 20: Loss = 14.032124519348145
Epoch: [0 / 1]  [   20/29316]  eta: 1:02:35  lr: 0.000000  size: 864  grad_norm: 55.2  loss_obj: 5.1620 (5.4035)  loss_cls: 5.2717 (4.9951)  loss_box: 0.8100 (0.8167)  losses: 14.3147 (14.4820)  time: 0.0712  data: 0.0002  max mem: 1295
Iter 21: Loss = 12.42043399810791
Iter 22: Loss = 14.724472045898438
Iter 23: Loss = 15.053595542907715
Iter 24: Loss = 13.350582122802734
Iter 25: Loss = 15.749482154846191
Iter 26: Loss = 12.529666900634766
Iter 27: Loss = 13.009824752807617
Iter 28: Loss = 14.51845932006836
Iter 29: Loss = 12.686975479125977
Iter 30: Loss = 14.82022476196289
Epoch: [0 / 1]  [   30/29316]  eta: 0:49:37  lr: 0.000000  size: 480  grad_norm: 65.1  loss_obj: 4.9561 (5.3599)  loss_cls: 4.7778 (4.9114)  loss_box: 0.7795 (0.8037)  losses: 14.1041 (14.2898)  time: 0.0538  data: 0.0002  max mem: 1356
Iter 31: Loss = 14.225656509399414
Iter 32: Loss = 11.492491722106934
Iter 33: Loss = 12.781495094299316
Iter 34: Loss = 12.16857624053955
Iter 35: Loss = 13.009968757629395
Iter 36: Loss = 16.088241577148438
Iter 37: Loss = 10.730937957763672
Iter 38: Loss = 13.108845710754395
Iter 39: Loss = 17.65089988708496
Iter 40: Loss = 12.07824993133545
Epoch: [0 / 1]  [   40/29316]  eta: 0:42:45  lr: 0.000000  size: 416  grad_norm: 32.5  loss_obj: 4.6318 (5.2635)  loss_cls: 4.6775 (4.8070)  loss_box: 0.7750 (0.7972)  losses: 13.0100 (14.0566)  time: 0.0450  data: 0.0002  max mem: 1356
Iter 41: Loss = 16.661056518554688
Iter 42: Loss = 20.347496032714844
Iter 43: Loss = 16.7158203125
Iter 44: Loss = 14.915279388427734
Iter 45: Loss = 12.017251968383789
Iter 46: Loss = 12.816083908081055
Iter 47: Loss = 12.810072898864746
Iter 48: Loss = 14.24181842803955
Iter 49: Loss = 11.81620979309082
Iter 50: Loss = 11.824243545532227
Epoch: [0 / 1]  [   50/29316]  eta: 0:38:13  lr: 0.000000  size: 384  grad_norm: 47.5  loss_obj: 4.5417 (5.3467)  loss_cls: 4.4998 (4.7960)  loss_box: 0.7806 (0.7969)  losses: 12.8161 (14.1272)  time: 0.0422  data: 0.0002  max mem: 1356
Iter 51: Loss = 12.442157745361328
Iter 52: Loss = 12.77001953125
Iter 53: Loss = 12.070394515991211
Iter 54: Loss = 13.451290130615234
Iter 55: Loss = 15.021827697753906
Iter 56: Loss = 16.284738540649414
Iter 57: Loss = 10.981136322021484
Iter 58: Loss = 11.875547409057617
Iter 59: Loss = 11.601882934570312
Iter 60: Loss = 14.507575988769531
Epoch: [0 / 1]  [   60/29316]  eta: 0:35:18  lr: 0.000000  size: 544  grad_norm: 56.0  loss_obj: 4.5971 (5.2638)  loss_cls: 4.3433 (4.7534)  loss_box: 0.7661 (0.7884)  losses: 12.8101 (13.9589)  time: 0.0412  data: 0.0003  max mem: 1356
Iter 61: Loss = 15.591327667236328
Iter 62: Loss = 11.733260154724121
Iter 63: Loss = 13.579202651977539
Iter 64: Loss = 15.732895851135254
Iter 65: Loss = 19.792924880981445
Iter 66: Loss = 12.672466278076172
Iter 67: Loss = 15.104918479919434
Iter 68: Loss = 16.080402374267578
Iter 69: Loss = 16.082782745361328
Iter 70: Loss = 12.52889633178711
Epoch: [0 / 1]  [   70/29316]  eta: 0:32:38  lr: 0.000000  size: 480  grad_norm: 41.3  loss_obj: 4.5971 (5.3145)  loss_cls: 4.7504 (4.8153)  loss_box: 0.7806 (0.7920)  losses: 13.4513 (14.0900)  time: 0.0379  data: 0.0003  max mem: 1356
Iter 71: Loss = 15.356693267822266
Iter 72: Loss = 16.66727638244629
Iter 73: Loss = 15.783282279968262
Iter 74: Loss = 12.908034324645996
Iter 75: Loss = 13.565144538879395
Iter 76: Loss = 14.785127639770508
Iter 77: Loss = 14.687912940979004
Iter 78: Loss = 13.662210464477539
Iter 79: Loss = 14.032866477966309
Iter 80: Loss = 14.126754760742188
Epoch: [0 / 1]  [   80/29316]  eta: 0:30:54  lr: 0.000000  size: 864  grad_norm: 38.2  loss_obj: 5.4628 (5.3605)  loss_cls: 5.0848 (4.8479)  loss_box: 0.7924 (0.7878)  losses: 14.6879 (14.1477)  time: 0.0361  data: 0.0018  max mem: 1356
Iter 81: Loss = 13.648673057556152
Iter 82: Loss = 12.843241691589355
Iter 83: Loss = 15.841216087341309
Iter 84: Loss = 12.831741333007812
Iter 85: Loss = 12.522369384765625
Iter 86: Loss = 15.202322006225586
Iter 87: Loss = 13.651979446411133
Iter 88: Loss = 13.194347381591797
Iter 89: Loss = 15.685914993286133
Iter 90: Loss = 11.11602783203125
Epoch: [0 / 1]  [   90/29316]  eta: 0:29:32  lr: 0.000000  size: 352  grad_norm: 40.2  loss_obj: 5.4342 (5.3224)  loss_cls: 4.7063 (4.8488)  loss_box: 0.7484 (0.7845)  losses: 13.6622 (14.0935)  time: 0.0383  data: 0.0037  max mem: 1356
Iter 91: Loss = 13.767312049865723
Iter 92: Loss = 13.47662353515625
Iter 93: Loss = 13.162429809570312
Iter 94: Loss = 12.938718795776367
Iter 95: Loss = 12.183754920959473
Iter 96: Loss = 14.643932342529297
Iter 97: Loss = 14.217443466186523
Iter 98: Loss = 15.801932334899902
Iter 99: Loss = 14.17972469329834
